import tensorflow as tf


def masked_softmax_cross_entropy(preds, labels, mask):
    """Softmax cross-entropy loss with masking."""
    # preds shape = [1,2]
    preds = preds[0] 
    labels = labels[0]
    #loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=preds, labels=labels)#tf.losses.sigmoid_cross_entropy(labels, preds) #calculate the loss
    loss = tf.losses.log_loss(labels, preds)
    #mask = tf.cast(mask, dtype=tf.float32) 
    #mask /= tf.reduce_mean(mask)
    #loss *= mask #multiply the loss by the mask to get only the effect of the masked data
    return tf.reduce_mean(loss) # return the mean of the loss (notice this is the operation to minimize)
 

def masked_accuracy(preds, labels, mask):
    """Accuracy with masking."""
    preds = preds[0]
    labels = labels[0]
    correct_prediction = tf.equal(tf.argmax(preds, 0), tf.argmax(labels, 0)) # find the max number of all classes in labels and preds
    # it will return the index with 1 in labels and the index with max number in preds and check their equality
    accuracy_all = tf.cast(correct_prediction, tf.float32)
    #mask = tf.cast(mask, dtype=tf.float32)
    #mask /= tf.reduce_mean(mask)
    #accuracy_all *= mask # multiply by the mask to get the effect of masked instances only
    return tf.reduce_mean(accuracy_all) # return the mean of the accuracy (notice this is the operation to minimize)
