from __future__ import division
from __future__ import print_function

import time
import tensorflow as tf

from utils import *
from models import GCN, MLP
import os
import random

# Set random seed
seed = 123 
np.random.seed(seed)
tf.set_random_seed(seed)

# Settings
# application flags are the command line arguments given to the program
flags = tf.app.flags # this line is just for easy access instead of writing tf.app.flags.DEFINE_string 
FLAGS = flags.FLAGS # get application flags (at this point there should be no flags)

flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'
flags.DEFINE_float('learning_rate', 0.002, 'Initial learning rate.') #0.001
flags.DEFINE_integer('epochs',1000 , 'Number of epochs to train.')
flags.DEFINE_integer('middle_layer_neurons', 32, 'Number of units in middle layer .') #209
flags.DEFINE_integer('output_layer', 16, 'Number of units in output layer.') #64
flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.') #32
flags.DEFINE_integer('hidden2', 64, 'Number of units in hidden layer 2.')
flags.DEFINE_integer('hidden3', 16, 'Number of units in hidden layer 3.')
flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')
flags.DEFINE_float('weight_decay', 0.005, 'Weight for L2 loss on embedding matrix.') #5e-4
flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')
flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')

# Initialize session
sess = tf.Session()

# Define model evaluation function
def evaluate(features, support, labels, mask, placeholders, dummyMain):
    t_test = time.time() # test start time
    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders) # calculate value to feed
                                                                                        # the placeholders
    feed_dict_val.update({placeholders['dummyMain']: dummyMain})
    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val) # run loss and accuracy operations
    # with feed_dict_val
    return outs_val[0], outs_val[1], (time.time() - t_test) # return loss, accuracy and time

model_func = GCN  # choose model GCN
num_supports = 1
numberOfFeatures = 209
placeholders = {
    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],  # for adjacency
    'features': tf.sparse_placeholder(tf.float32, shape=(None,numberOfFeatures)),  # for features
    'labels': tf.placeholder(tf.float32, shape=(None, 2)),  # for labels
    'labels_mask': tf.placeholder(tf.int32),  # for mask
    'dropout': tf.placeholder_with_default(0., shape=()),  # for dropout
    'num_features_nonzero': tf.placeholder(tf.int32),  # helper variable for sparse dropout
    'dummyMain':tf.placeholder(tf.float32, shape=(209))
}
# Create model
model = model_func(placeholders, input_dim=numberOfFeatures, logging=True)

sess.run(tf.global_variables_initializer())
#model.load(sess)
highest_accuracy = 0 #93 #93.85 #95.8
#'''
apps = os.listdir("training_data")
random.seed(14) #17
random.shuffle(apps)
highest_training_accuracy = 0 #94.82 #90.65 #91.7
current_learning_rate = FLAGS.learning_rate
#current_size = 1500
count = 0
random.shuffle(apps)
current_apps = apps
train_data_length = len(current_apps)
random.shuffle(current_apps)
current_index = 0
learning_rates = [0.002, 0.001, 0.0005, 0.0002, 0.0001]
for epoch in range(FLAGS.epochs):
    correct = 0
    #random.shuffle(apps)
    z = 0
    #current_size += 500

    #while count < 10:
    correct = 0
    random.shuffle(current_apps)
    #print(current_learning_rate)
    #model.optimizer = tf.train.AdamOptimizer(learning_rate=current_learning_rate)
    max_loss = 0
    total_loss = 0
    next_apps = []
    if(len(next_apps) != 0):
        current_apps = next_apps
    for appName in current_apps:
        z += 1
        #print("Epoch:" + str(epoch+1) + " app number " + str(z))
        adj, features, y_train, train_mask = load_data(os.path.join('training_data',appName))
        # Some preprocessing
        dummyMain = (features[0].toarray())[0].tolist()
        features = preprocess_features(features)
        support = [preprocess_adj(adj)]  # support = D^-1/2 * A * D^-1/2

        # Construct feed dictionary
        feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)
        feed_dict.update({placeholders['dropout']: FLAGS.dropout})
        feed_dict.update({placeholders['dummyMain']: dummyMain})

        # Training step
        # run optimization , loss and accuracy operations with given feed dictionary

        outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)
        if(outs[2] == 1):
            #print("Epoch:" + str(epoch + 1) + " app number " + str(z) + ' detected correctly' + ' loss=' + str(outs[1]))
            correct += 1
        #else:
            #next_apps.append(appName)
            #print("Epoch:" + str(epoch + 1) + " app number " + str(z) + ' detected incorrectly'+ ' loss=' + str(outs[1]))
        if(outs[1] > max_loss):
            max_loss = outs[1]
        total_loss += outs[1]
    # adaptive learning rate
    #if(correct/train_data_length * 100 >= highest_training_accuracy):
        #highest_training_accuracy = correct/train_data_length * 100
        #count = 0
        #model.save(sess)
    #else:
        #count += 1
    #if(count == 15):
        #current_learning_rate /= 2
        #model.optimizer = tf.train.AdamOptimizer(learning_rate=current_learning_rate)
        #count = 0
        #print('current learning_rate = ' + str(current_learning_rate))
    #print('current_size = ' + str(current_size))
    #print('current_applications_number = ' + str(len(current_apps)))
    print("Epoch:" + str(epoch+1) + "  train_accuracy = " + str(correct/len(current_apps) * 100) + ' %') #train_data_length
    print('avg_loss = ' + str(total_loss/len(current_apps)) + ' max_loss = ' + str(max_loss))
    #model.save(sess)
    # learning rate schedule
    if(correct/len(current_apps) * 100 >= highest_training_accuracy):
        #model.save(sess)
        highest_training_accuracy = correct/len(current_apps) * 100
        count = 0
    else:
        count += 1
    if count == 5:
        count = 0
        current_index = (current_index + 1) % len(learning_rates)
        model.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rates[current_index])
        model.load(sess)
        print('current_learning_rate = ' + str(learning_rates[current_index]))
    #current_apps = next_apps
    #current_learning_rate = 1.1 * current_learning_rate
    #'''
    total_test_data = len(os.listdir('test_data')) #1000
    predicted_correctly = 0
    max_cost = 0
    total_cost = 0
    for appName in os.listdir("test_data"):
        adj, features, y_test, test_mask = load_data(os.path.join('test_data', appName))
        dummyMain = (features[0].toarray())[0].tolist()
        features = preprocess_features(features)
        support = [preprocess_adj(adj)]  # support = D^-1/2 * A * D^-1/2
        test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders, dummyMain)
        predicted_correctly += test_acc
        if(test_cost > max_cost):
            max_cost = test_cost
        total_cost += test_cost
        #print("Test set results:", "cost=", "{:.5f}".format(test_cost),
          #"test_accuracy=", "{:.5f}".format(test_acc), "time=", "{:.5f}".format(test_duration))
    print('test_accuracy = ' + str(predicted_correctly/total_test_data * 100) + ' %')
    print('avg_cost = ' + str(total_cost / total_test_data) + ' max_cost = ' + str(max_cost))
    
    #model.save(sess)
    if (predicted_correctly/total_test_data * 100) >= highest_accuracy:
        model.save(sess)
        highest_accuracy = predicted_correctly/total_test_data * 100
        #count = 0;
    #else:
        #count += 1
    #'''
print("Optimization Finished!")
#'''

# Testing

total_test_data = len(os.listdir('test_data')) #1000
predicted_correctly = 0
malwareFalseDetection = 0
benginFalseDetection = 0
for appName in os.listdir("test_data"):
    adj, features, y_test, test_mask = load_data(os.path.join('test_data', appName))
    dummyMain = (features[0].toarray())[0].tolist()
    features = preprocess_features(features)
    support = [preprocess_adj(adj)]  # support = D^-1/2 * A * D^-1/2
    test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders, dummyMain)
    predicted_correctly += test_acc
    feed_dict = dict() # make and empty dictionary
    feed_dict.update({placeholders['features']: features}) # add features as value for placeholders['features']
    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})
    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape}) # number of features for each node
    feed_dict.update({placeholders['dummyMain']: dummyMain})
    out = sess.run(model.predict(), feed_dict=feed_dict)[0]
    if(test_acc == 1):
        print("Test set results:", appName ," detected correctly " + 'cost = ' + str(test_cost))
    else:
        #print(out)
        if (out[0] >= out[1]):
            malwareFalseDetection += 1
        else:
            benginFalseDetection += 1
        print("Test set results:", appName, " detected incorrectly " + 'cost = ' + str(test_cost))


print('accuracy = ' + str(predicted_correctly/total_test_data * 100) + ' %')
#print('bengin false detection = ' + str(benginFalseDetection))
#print('malware false detection = ' + str(malwareFalseDetection))

'''
model.load(sess)
for appName in os.listdir("test_data"):
    adj, features, y_test, test_mask = load_data(os.path.join('test_data', appName))
    features = preprocess_features(features)
    support = [preprocess_adj(adj)]  # support = D^-1/2 * A * D^-1/2
    feed_dict = dict() # make and empty dictionary
    feed_dict.update({placeholders['features']: features}) # add features as value for placeholders['features']
    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})
    # add support as value for placeholders['support']
    # notice support is an array of adjacency matrices, that is the reason for the loop
    # this means we can give multiple graphs for the same features(this feature won't be needed in our network
    #so we will always keep support_num = 1)
    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape}) # number of features for each node
    out = sess.run(model.predict(), feed_dict=feed_dict)[0]
    if(out[0] >= out[1]):
        print(appName + ' ' + 'malware')
    else:
        print(appName + ' ' + 'bengin')
    print(out)
    #test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)
    #predicted_correctly += test_acc
    #print("Test set results:", "cost=", "{:.5f}".format(test_cost),
     # "accuracy=", "{:.5f}".format(test_acc), "time=", "{:.5f}".format(test_duration))
'''

